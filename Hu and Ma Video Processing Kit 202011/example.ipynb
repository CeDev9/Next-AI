{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Processing Example\n",
    "\n",
    "This example shows how to use `interactionvideo` package to process a video for studies in human interactions. Please also refer to our research paper: Hu and Ma (2020), \"Persuading Investors: A Video-Based Study\", available at: https://songma.github.io/files/hm_video.pdf.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The video processing involves the following steps:\n",
    "1. Set up folders and check dependencies (requirements)\n",
    "2. Extract images and audios from a video using `pliers`\n",
    "3. Extract text from audios using Google Speech2Text API\n",
    "4. Process images(faces) using Face++ API\n",
    "5. Process text using Loughran and McDonald (2011) Finance Dictionary and Nicolas, Bai, and Fiske (2019) Social Psychology Dictionary\n",
    "6. Process audios using pre-trained ML models in `pyAudioAnalysis` and `speechemotionrecognition`\n",
    "7. Aggregate information from 3V (visual, vocal, and verbal) to video level\n",
    "\n",
    "## Structure\n",
    "\n",
    "```bash\n",
    "├── interactionvideo\n",
    "│   ├── __pycache__\n",
    "│   ├── prepare.py\n",
    "│   ├── decompose.py\n",
    "│   ├── faceppml.py\n",
    "│   ├── googleml.py\n",
    "│   ├── textualanalysis.py\n",
    "│   ├── audioml.py\n",
    "│   ├── aggregate.py\n",
    "│   └── utils.py\n",
    "├── data\n",
    "│   ├── example_video.mp4\n",
    "│   └── VideoDictionary.csv\n",
    "├── mlmodel\n",
    "│   ├── pyAudioAnalysis\n",
    "│   └── speechemotionrecognition\n",
    "├── output\n",
    "│   ├── audio_temp\n",
    "│   ├── image_temp\n",
    "│   └── result_temp\n",
    "├── PythonSDK\n",
    "├── example.py\n",
    "├── Video Processing Example.ipynb\n",
    "├── README.md\n",
    "└── requirement.txt\n",
    "```\n",
    "\n",
    "## Dependencies\n",
    " - pandas \n",
    " - tqdm \n",
    " - codecs\n",
    " - pliers\n",
    " - pydub\n",
    " - PIL\n",
    " - google-cloud-speech\n",
    " - google-cloud-storage\n",
    " - speechemotionrecognition\n",
    " - pyAudioAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up folders and check dependencies (requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "# Set your root path here\n",
    "RootPath = r''\n",
    "# Set your video file path here\n",
    "VideoFilePath = join(RootPath,'data','example_video.mp4')\n",
    "# Set your work path here\n",
    "# Work path is where to store meta files and output files\n",
    "WorkPath = join(RootPath,'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the folders\n",
    "from interactionvideo.prepare import setup_folder\n",
    "setup_folder(WorkPath)\n",
    "\n",
    "# check the requirements for interactionvideo\n",
    "from interactionvideo.prepare import check_requirements\n",
    "check_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract images and audios from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.decompose import convert_video_to_images\n",
    "\n",
    "# Decompose the video into a stream of images\n",
    "# The default sampling rate is 10 frames per second\n",
    "# Find the output at WorkPath\\image_temp\n",
    "convert_video_to_images(VideoFilePath, WorkPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.decompose import convert_video_to_audios\n",
    "\n",
    "# Decompose the video into audios\n",
    "# Find the output at WorkPath\\audio_temp\n",
    "convert_video_to_audios(VideoFilePath, WorkPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract text from audios using Google Speech2Text API\n",
    "\n",
    "Set up your Google Cloud environment following\n",
    "\n",
    " - https://cloud.google.com/python\n",
    " - https://cloud.google.com/storage/docs/quickstart-console\n",
    " - https://cloud.google.com/speech-to-text\n",
    "\n",
    "Create a Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.googleml import upload_audio_to_googlecloud\n",
    "\n",
    "# Set your Google Cloud Storage bucket name here\n",
    "GoogleBucketName = ''\n",
    "\n",
    "# Upload audio file to Google Cloud Storage\n",
    "upload_audio_to_googlecloud(WorkPath, GoogleBucketName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.googleml import convert_audio_to_text_by_google\n",
    "\n",
    "# Use Google Speech2Text API to convert audio to text\n",
    "# Return a txt file of full speech script and a csv file of text and punctuation\n",
    "# Find the output at \n",
    "# - WorkPath\\result_temp\\script_google.txt (full speech script)\n",
    "# - WorkPath\\result_temp\\text_panel_google.csv (text panel from Google)\n",
    "google_result_text, google_result_df = convert_audio_to_text_by_google(WorkPath, GoogleBucketName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check full speech script from Google\n",
    "print(google_result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check text panel from Google\n",
    "google_result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process images(faces) using Face++ API\n",
    "\n",
    "Get your key and secret from https://www.faceplusplus.com.\n",
    "\n",
    "If you register at https://console.faceplusplus.com/register, use https://api-us.faceplusplus.com as the server.\n",
    "\n",
    "If you register at https://console.faceplusplus.com.cn/register, use https://api-cn.faceplusplus.com as the server.\n",
    "\n",
    "The `Python SDK` of Face++ is included in this package.\n",
    "\n",
    "You can also download it from https://github.com/FacePlusPlus/facepp-python-sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.faceppml import process_image_by_facepp\n",
    "\n",
    "# Use Face++ ML API to process images\n",
    "# Return csv files of facial emotion, gender, predicted age\n",
    "# Find the output\n",
    "# - WorkPath\\result_temp\\face_panel_facepp.csv (full returns from Face++)\n",
    "# - WorkPath\\result_temp\\face_panel.csv (clean results)\n",
    "\n",
    "# Set your key, secret, and server here\n",
    "FaceppKey = ''\n",
    "FaceppSecret = ''\n",
    "FaceppServer = 'https://api-us.faceplusplus.com'\n",
    "\n",
    "facepp_result_df, facepp_result_clean_df = process_image_by_facepp(VideoFilePath, WorkPath,\\\n",
    "                                            FaceppKey, FaceppSecret, FaceppServer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check full returns from Face++\n",
    "facepp_result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check clean results\n",
    "facepp_result_clean_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process text using LM and NBF Dictionaries\n",
    "\n",
    "Use Loughran-McDonald (2011) Finance Dictionary (LM) to construct verbal positive and negative.\n",
    "\n",
    "For more details, please check https://sraf.nd.edu/textual-analysis/resources.\n",
    "\n",
    "Use Nicolas, Bai, and Fiske (2019) Social Psychology Dictionary (NBF) to construct verbal warmth and ability.\n",
    "\n",
    "For more details, please check https://psyarxiv.com/afm8k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.textualanalysis import process_text_by_dict\n",
    "\n",
    "# Set LM-NBF dictionary path\n",
    "DictionaryPath = join(RootPath,'data','VideoDictionary.csv')\n",
    "\n",
    "# Dictionary-based textual analysis to get verbal measures\n",
    "# (e.g., verbal positive, negative, warmth, ability)\n",
    "# Return csv files of verbal positive, negative, warmth, and ability\n",
    "# Find the output at \n",
    "# - WorkPath\\result_temp\\text_panel.csv\n",
    "text_result_df = process_text_by_dict(WorkPath, DictionaryPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check text panel from Dictionary\n",
    "text_result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process audios by pre-trained ML models\n",
    "\n",
    "Construct vocal arousal and vocal valence from pre-trained SVM ML models in `pyAudioAnalysis`.\n",
    "\n",
    "The pre-trained models are located at mlmodel\\pyAudioAnalysis\n",
    "- svmSpeechEmotion_arousal\n",
    "- svmSpeechEmotion_arousalMEANS\n",
    "- svmSpeechEmotion_valence\n",
    "- svmSpeechEmotion_valenceMEANS\n",
    "\n",
    "For more details, please check https://github.com/tyiannak/pyAudioAnalysis/wiki/4.-Classification-and-Regression.\n",
    "\n",
    "Construct vocal positive and vocal negative from pre-trained LSTM ML models in `speechemotionrecognition`.\n",
    "\n",
    "The pre-trained models are located at mlmodel\\speechemotionrecognition\n",
    "- best_model_LSTM_39.h5\n",
    "\n",
    "For more details, please check https://github.com/harry-7/speech-emotion-recognition.\n",
    "\n",
    "Note: speechemotionrecognition requires tensorflow and Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.audioml import process_audio_by_pyAudioAnalysis\n",
    "\n",
    "# Set the model path\n",
    "pyAudioAnalysisModelPath = join(RootPath,'mlmodel','pyAudioAnalysis')\n",
    "\n",
    "# Construct vocal arousal and vocal valence\n",
    "# Find the output at \n",
    "# - WorkPath\\result_temp\\audio_panel_pyAudioAnalysis.csv\n",
    "audio_result_df1 = process_audio_by_pyAudioAnalysis(WorkPath, pyAudioAnalysisModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check audio panel from pyAudioAnalysis\n",
    "audio_result_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.audioml import process_audio_by_speechemotionrecognition\n",
    "\n",
    "# Set the model path\n",
    "speechemotionrecognitionModelPath = join(RootPath,'mlmodel','speechemotionrecognition')\n",
    "\n",
    "# Construct vocal positive and vocal negative\n",
    "# Find the output at \n",
    "# - WorkPath\\result_temp\\audio_panel_speechemotionrecognition.csv\n",
    "audio_result_df2 = process_audio_by_speechemotionrecognition(WorkPath, speechemotionrecognitionModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check audio panel from speechemotionrecognition\n",
    "audio_result_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Aggregate information from 3V to video level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactionvideo.aggregate import aggregate_3v_to_video\n",
    "\n",
    "# Aggregate 3V information\n",
    "# Find the output at \n",
    "# - WorkPath\\result_temp\\video_panel.csv\n",
    "video_result_df = aggregate_3v_to_video(WorkPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check video panel\n",
    "video_result_df.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}